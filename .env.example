# ============================================
# AWS Configuration
# ============================================
# Required: Your AWS credentials
AWS_ACCESS_KEY_ID=your_aws_access_key_id_here
AWS_SECRET_ACCESS_KEY=your_aws_secret_access_key_here
AWS_REGION=us-east-1

# ============================================
# S3 Configuration
# ============================================
# Source bucket name (where CSV files are uploaded)
# Leave empty to use Terraform-generated bucket name
SOURCE_BUCKET=etl-pipeline-source-dev

# Destination bucket name (where processed Parquet files are stored)
# Leave empty to use Terraform-generated bucket name
DESTINATION_BUCKET=etl-pipeline-output-dev

# Source path (for local testing)
# Use S3 URI format: s3://bucket-name/input/file.csv
# Or local path: data/your_file.csv
SOURCE_PATH=s3://your-source-bucket/input/your_file.csv

# Source type: 's3' or 'local'
SOURCE_TYPE=s3

# Output prefix for processed data in destination bucket
OUTPUT_PREFIX=processed_data

# ============================================
# Spark Configuration
# ============================================
# Spark master URL (for local execution)
SPARK_MASTER=local[*]

# Spark driver memory
SPARK_DRIVER_MEMORY=2g

# Spark executor memory
SPARK_EXECUTOR_MEMORY=2g

# ============================================
# Monitoring & Logging
# ============================================
# CloudWatch log group name
CLOUDWATCH_LOG_GROUP=etl-pipeline

# Enable CloudWatch logging (true/false)
CLOUDWATCH_ENABLED=true

# ============================================
# Monitoring & Logging
# ============================================
# CloudWatch log group name (for local execution only)
# Lambda gets this automatically from Terraform
CLOUDWATCH_LOG_GROUP=etl-pipeline

# Enable CloudWatch logging (true/false)
# Used by Terraform: maps to TF_VAR_enable_cloudwatch
# Lambda gets this automatically from Terraform variable
ENABLE_CLOUDWATCH=true

# ============================================
# Notes
# ============================================
# 1. For Lambda function: The following environment variables are automatically set by Terraform:
#    - GLUE_JOB_NAME (from Glue job resource)
#    - DESTINATION_BUCKET (from destination S3 bucket)
#    - OUTPUT_PREFIX (hardcoded to "processed_data")
#    - CLOUDWATCH_LOG_GROUP (from CloudWatch log group or default)
#    - CLOUDWATCH_ENABLED (from enable_cloudwatch Terraform variable)
#    Note: AWS_REGION is available from Lambda runtime environment, not set explicitly
#
# 2. Variables used by Terraform (via terraform.ps1):
#    - AWS_REGION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
#    - SOURCE_BUCKET, DESTINATION_BUCKET
#    - ENVIRONMENT, PROJECT_NAME, ENABLE_CLOUDWATCH
#    - GLUE_SCRIPTS_BUCKET_NAME (optional)
#
# 3. Variables for local execution only:
#    - SOURCE_PATH, SOURCE_TYPE, OUTPUT_PREFIX
#    - CLOUDWATCH_LOG_GROUP, SPARK_* variables
#
# 4. After Terraform deployment, get bucket names with:
#    terraform output source_bucket_name
#    terraform output destination_bucket_name


